# models/opensource/qwen/qwen_wrapper.py

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from typing import List, Dict

class QwenModelWrapper:
    def __init__(self, model_path: str, config: Dict = None):
        """
        åˆå§‹åŒ–Qwenæ¨¡å‹å’Œåˆ†è©å™¨
        """
        print(f"Initializing Qwen model from: {model_path}")
        self.model_path = model_path
        self.config = config if isinstance(config, dict) else {}

        # æª¢æ¸¬å¯ç”¨çš„è¨­å‚™ (CUDA GPU æˆ– CPU)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Target device set to: {self.device}")

        # è¼‰å…¥åˆ†è©å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )

        # --- ğŸ”´ é—œéµä¿®æ­£ï¼šæ¡ç”¨æ›´ç›´æ¥çš„æ–¹å¼å°‡æ¨¡å‹åŠ è¼‰åˆ° GPU ---
        print("Loading model to CPU first...")
        # 1. å…ˆåœ¨ CPU ä¸Šä»¥ bfloat16 æ ¼å¼åŠ è¼‰æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
        # 2. ç„¶å¾Œï¼Œå°‡æ•´å€‹æ¨¡å‹å¼·åˆ¶ç§»å‹•åˆ°ç›®æ¨™è¨­å‚™ (GPU)
        print(f"Moving model to {self.device}...")
        self.model.to(self.device)
        # ----------------------------------------------------
        
        self.model.eval() # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
        print(f"Model successfully loaded and moved to {self.device}.")

    def chat(self, query: str, history: List[Dict[str, str]] = None) -> str:
        """
        èˆ‡æ¨¡å‹é€²è¡Œå°è©±çš„æ ¸å¿ƒæ–¹æ³•
        """
        if history is None:
            messages = [{"role": "user", "content": query}]
        else:
            messages = history + [{"role": "user", "content": query}]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        model_inputs = self.tokenizer([text], return_tensors="pt")

        # --- ğŸ”´ åŒæ¨£é—œéµï¼šç¢ºä¿è¼¸å…¥å¼µé‡ä¹Ÿè¢«ç§»å‹•åˆ°èˆ‡æ¨¡å‹ç›¸åŒçš„è¨­å‚™ ---
        model_inputs = model_inputs.to(self.device)
        
        model_params = self.config.get("model_params", {})
        max_new_tokens = model_params.get("max_tokens", 1024)
        temperature = model_params.get("temperature", 0.6)
        top_p = model_params.get("top_p", 0.9)

        generated_ids = self.model.generate(
            model_inputs.input_ids,
            attention_mask=model_inputs.attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

        return response.strip()

# ç”¨æ–¼å–®ç¨æ¸¬è©¦çš„ç¤ºä¾‹
if __name__ == '__main__':
    print("QwenModelWrapper is defined correctly. This file can be imported by other scripts.")